"""
å®Ÿé¨“çµæœã®å¯è¦–åŒ–ã¨çµ±è¨ˆåˆ†æ
"""

import numpy as np
import matplotlib.pyplot as plt
import japanize_matplotlib  # æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆå¯¾å¿œ
from scipy.stats import ttest_rel
import json
import os
from typing import Dict, List, Any


def convert_to_serializable(obj):
    """
    numpyå‹ã‚’JSON serializable ãªå‹ã«å¤‰æ›
    
    Args:
        obj: å¤‰æ›ã™ã‚‹å¯¾è±¡ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
    
    Returns:
        JSON serializable ãªã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
    """
    if isinstance(obj, np.bool_):
        return bool(obj)
    elif isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, dict):
        return {key: convert_to_serializable(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [convert_to_serializable(item) for item in obj]
    else:
        return obj


def calculate_statistics(results: Dict[str, Dict[str, List[float]]]) -> Dict:
    """
    çµæœã®çµ±è¨ˆé‡ã‚’è¨ˆç®—
    
    Args:
        results: ãƒ¢ãƒ‡ãƒ«ã”ã¨ã®è©•ä¾¡çµæœ
    
    Returns:
        stats: çµ±è¨ˆé‡
    """
    stats = {}
    
    for model_name, metrics in results.items():
        stats[model_name] = {}
        for metric_name, values in metrics.items():
            if len(values) > 0:
                stats[model_name][metric_name] = {
                    'mean': float(np.mean(values)),
                    'std': float(np.std(values)),
                    'min': float(np.min(values)),
                    'max': float(np.max(values)),
                    'ci_lower': float(np.percentile(values, 2.5)),
                    'ci_upper': float(np.percentile(values, 97.5))
                }
            else:
                stats[model_name][metric_name] = {
                    'mean': 0.0,
                    'std': 0.0,
                    'min': 0.0,
                    'max': 0.0,
                    'ci_lower': 0.0,
                    'ci_upper': 0.0
                }
    
    return stats


def perform_statistical_tests(results: Dict[str, Dict[str, List[float]]]) -> Dict:
    """
    çµ±è¨ˆçš„æœ‰æ„æ€§æ¤œå®šã‚’å®Ÿè¡Œï¼ˆå¯¾å¿œã®ã‚ã‚‹tæ¤œå®šï¼‰
    
    Args:
        results: ãƒ¢ãƒ‡ãƒ«ã”ã¨ã®è©•ä¾¡çµæœ
    
    Returns:
        test_results: æ¤œå®šçµæœ
    """
    test_results = {}
    models = list(results.keys())
    
    for i, model1 in enumerate(models):
        for j, model2 in enumerate(models[i+1:], i+1):
            test_results[f"{model1}_vs_{model2}"] = {}
            
            for metric in ['accuracy', 'f1', 'auc']:
                values1 = results[model1][metric]
                values2 = results[model2][metric]
                
                # ãƒ‡ãƒ¼ã‚¿é•·ãŒåŒã˜ã‹ãƒã‚§ãƒƒã‚¯
                min_len = min(len(values1), len(values2))
                if min_len < 2:
                    test_results[f"{model1}_vs_{model2}"][metric] = {
                        'statistic': np.nan,
                        'p_value': np.nan,
                        'significant': False
                    }
                    continue
                
                # é…åˆ—ã‚’åŒã˜é•·ã•ã«èª¿æ•´
                values1_trimmed = np.array(values1[:min_len])
                values2_trimmed = np.array(values2[:min_len])
                
                try:
                    # å¯¾å¿œã®ã‚ã‚‹tæ¤œå®š
                    statistic, p_value = ttest_rel(values1_trimmed, values2_trimmed)
                    test_results[f"{model1}_vs_{model2}"][metric] = {
                        'statistic': float(statistic),
                        'p_value': float(p_value),
                        'significant': p_value < 0.05
                    }
                except Exception as e:
                    test_results[f"{model1}_vs_{model2}"][metric] = {
                        'statistic': np.nan,
                        'p_value': np.nan,
                        'significant': False
                    }
    
    return test_results


def plot_training_curves(train_losses: List[float], val_losses: List[float] = None,
                         model_name: str = "Model", save_path: str = None,
                         show_plot: bool = True):
    """
    å­¦ç¿’æ›²ç·šã‚’ãƒ—ãƒ­ãƒƒãƒˆ
    
    Args:
        train_losses: è¨“ç·´æå¤±ã®ãƒªã‚¹ãƒˆ
        val_losses: æ¤œè¨¼æå¤±ã®ãƒªã‚¹ãƒˆ
        model_name: ãƒ¢ãƒ‡ãƒ«å
        save_path: ä¿å­˜å…ˆãƒ‘ã‚¹
        show_plot: ãƒ—ãƒ­ãƒƒãƒˆã‚’è¡¨ç¤ºã™ã‚‹ã‹
    """
    plt.figure(figsize=(10, 6))
    
    epochs = range(1, len(train_losses) + 1)
    plt.plot(epochs, train_losses, 'b-', label='Training Loss', linewidth=2)
    
    if val_losses is not None and len(val_losses) > 0:
        plt.plot(epochs, val_losses, 'r-', label='Validation Loss', linewidth=2)
    
    plt.title(f'{model_name} - å­¦ç¿’æ›²ç·š', fontsize=14)
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # æœ€çµ‚æå¤±å€¤ã‚’è¡¨ç¤º
    if train_losses:
        final_train_loss = train_losses[-1]
        plt.text(0.02, 0.98, f'æœ€çµ‚è¨“ç·´æå¤±: {final_train_loss:.4f}', 
                 transform=plt.gca().transAxes, verticalalignment='top',
                 bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))
    
    if val_losses and len(val_losses) > 0:
        final_val_loss = val_losses[-1]
        plt.text(0.02, 0.88, f'æœ€çµ‚æ¤œè¨¼æå¤±: {final_val_loss:.4f}', 
                 transform=plt.gca().transAxes, verticalalignment='top',
                 bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.8))
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"ğŸ“Š å­¦ç¿’æ›²ç·šã‚’ä¿å­˜: {save_path}")
    
    if show_plot:
        plt.show()
    else:
        plt.close()


def plot_box_plots(results: Dict[str, Dict[str, List[float]]],
                   save_path: str = None, show_plot: bool = True):
    """
    ãƒœãƒƒã‚¯ã‚¹ãƒ—ãƒ­ãƒƒãƒˆã§çµæœã‚’å¯è¦–åŒ–
    
    Args:
        results: ãƒ¢ãƒ‡ãƒ«ã”ã¨ã®è©•ä¾¡çµæœ
        save_path: ä¿å­˜å…ˆãƒ‘ã‚¹
        show_plot: ãƒ—ãƒ­ãƒƒãƒˆã‚’è¡¨ç¤ºã™ã‚‹ã‹
    """
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    fig.suptitle('5-fold Cross-Validation çµæœ', fontsize=16)
    
    metrics_to_plot = ['accuracy', 'precision', 'recall', 'f1']
    metric_labels = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
    
    for idx, (metric, label) in enumerate(zip(metrics_to_plot, metric_labels)):
        ax = axes[idx // 2, idx % 2]
        
        # ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
        model_names = list(results.keys())
        values = [results[model][metric] for model in model_names]
        
        # ãƒœãƒƒã‚¯ã‚¹ãƒ—ãƒ­ãƒƒãƒˆ
        box_plot = ax.boxplot(values, labels=model_names, patch_artist=True)
        
        # è‰²ã‚’è¨­å®š
        colors = plt.cm.Set3(np.linspace(0, 1, len(model_names)))
        for patch, color in zip(box_plot['boxes'], colors):
            patch.set_facecolor(color)
        
        ax.set_title(f'{label} ã®åˆ†å¸ƒ')
        ax.set_ylabel(label)
        ax.grid(True, alpha=0.3)
        ax.tick_params(axis='x', rotation=45)
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"ğŸ“Š ãƒœãƒƒã‚¯ã‚¹ãƒ—ãƒ­ãƒƒãƒˆã‚’ä¿å­˜: {save_path}")
    
    if show_plot:
        plt.show()
    else:
        plt.close()


def plot_bar_charts(stats: Dict, save_path: str = None, show_plot: bool = True):
    """
    æ£’ã‚°ãƒ©ãƒ•ã§AUCã‚¹ã‚³ã‚¢ã‚’æ¯”è¼ƒ
    
    Args:
        stats: çµ±è¨ˆé‡
        save_path: ä¿å­˜å…ˆãƒ‘ã‚¹
        show_plot: ãƒ—ãƒ­ãƒƒãƒˆã‚’è¡¨ç¤ºã™ã‚‹ã‹
    """
    plt.figure(figsize=(12, 6))
    
    model_names = list(stats.keys())
    auc_means = [stats[model]['auc']['mean'] for model in model_names]
    auc_stds = [stats[model]['auc']['std'] for model in model_names]
    
    # è‰²è¨­å®š
    colors = plt.cm.Set2(np.linspace(0, 1, len(model_names)))
    bars = plt.bar(model_names, auc_means, yerr=auc_stds, capsize=5, 
                   alpha=0.7, color=colors)
    
    # æœ€é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«ã‚’å¼·èª¿
    best_idx = np.argmax(auc_means)
    bars[best_idx].set_edgecolor('red')
    bars[best_idx].set_linewidth(3)
    
    plt.title('AUC ã‚¹ã‚³ã‚¢æ¯”è¼ƒ (å¹³å‡ Â± æ¨™æº–åå·®)', fontsize=14)
    plt.ylabel('AUC')
    plt.ylim(0, 1.0)
    plt.grid(True, alpha=0.3, axis='y')
    plt.xticks(rotation=45)
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"ğŸ“Š æ£’ã‚°ãƒ©ãƒ•ã‚’ä¿å­˜: {save_path}")
    
    if show_plot:
        plt.show()
    else:
        plt.close()


def plot_comprehensive_analysis(results: Dict[str, Dict[str, List[float]]],
                                save_path: str = None, show_plot: bool = True):
    """
    åŒ…æ‹¬çš„ãªåˆ†æãƒ—ãƒ­ãƒƒãƒˆ
    
    Args:
        results: ãƒ¢ãƒ‡ãƒ«ã”ã¨ã®è©•ä¾¡çµæœ
        save_path: ä¿å­˜å…ˆãƒ‘ã‚¹
        show_plot: ãƒ—ãƒ­ãƒƒãƒˆã‚’è¡¨ç¤ºã™ã‚‹ã‹
    """
    plt.figure(figsize=(15, 10))
    
    models = list(results.keys())
    
    # ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆ1: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ
    plt.subplot(2, 3, 1)
    accuracies = [np.mean(results[model]['accuracy']) for model in models]
    accuracy_stds = [np.std(results[model]['accuracy']) for model in models]
    
    bars = plt.bar(models, accuracies, yerr=accuracy_stds, capsize=5, alpha=0.7)
    plt.title('ãƒ¢ãƒ‡ãƒ«ç²¾åº¦æ¯”è¼ƒ')
    plt.ylabel('Accuracy')
    plt.xticks(rotation=45)
    plt.grid(True, alpha=0.3)
    
    # æœ€é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«ã‚’å¼·èª¿
    best_idx = np.argmax(accuracies)
    bars[best_idx].set_color('gold')
    bars[best_idx].set_edgecolor('red')
    bars[best_idx].set_linewidth(2)
    
    # ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆ2: F1ã‚¹ã‚³ã‚¢æ¯”è¼ƒ
    plt.subplot(2, 3, 2)
    f1_scores = [np.mean(results[model]['f1']) for model in models]
    f1_stds = [np.std(results[model]['f1']) for model in models]
    
    plt.bar(models, f1_scores, yerr=f1_stds, capsize=5, alpha=0.7, color='lightgreen')
    plt.title('F1ã‚¹ã‚³ã‚¢æ¯”è¼ƒ')
    plt.ylabel('F1 Score')
    plt.xticks(rotation=45)
    plt.grid(True, alpha=0.3)
    
    # ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆ3: AUCæ¯”è¼ƒ
    plt.subplot(2, 3, 3)
    auc_scores = [np.mean(results[model]['auc']) for model in models]
    auc_stds = [np.std(results[model]['auc']) for model in models]
    
    plt.bar(models, auc_scores, yerr=auc_stds, capsize=5, alpha=0.7, color='lightcoral')
    plt.title('AUCã‚¹ã‚³ã‚¢æ¯”è¼ƒ')
    plt.ylabel('AUC')
    plt.xticks(rotation=45)
    plt.grid(True, alpha=0.3)
    
    # ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆ4: ç²¾åº¦åˆ†å¸ƒã®ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ 
    plt.subplot(2, 3, 4)
    for i, model in enumerate(models):
        plt.hist(results[model]['accuracy'], alpha=0.6, label=model, bins=5)
    plt.title('ç²¾åº¦åˆ†å¸ƒ')
    plt.xlabel('Accuracy')
    plt.ylabel('é »åº¦')
    plt.legend(fontsize=8)
    plt.grid(True, alpha=0.3)
    
    # ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆ5: å„ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ã§ã®æ€§èƒ½å¤‰å‹•
    plt.subplot(2, 3, 5)
    n_folds = len(results[models[0]]['accuracy'])
    folds = range(1, n_folds + 1)
    for model in models:
        plt.plot(folds, results[model]['accuracy'], 'o-', label=model, alpha=0.7)
    plt.title('ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰é–“ã®ç²¾åº¦å¤‰å‹•')
    plt.xlabel('Fold')
    plt.ylabel('Accuracy')
    plt.legend(fontsize=8)
    plt.grid(True, alpha=0.3)
    
    # ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆ6: ç²¾åº¦ vs F1 ã®æ•£å¸ƒå›³
    plt.subplot(2, 3, 6)
    for model in models:
        acc_values = results[model]['accuracy']
        f1_values = results[model]['f1']
        plt.scatter(acc_values, f1_values, label=model, alpha=0.7, s=50)
    plt.title('ç²¾åº¦ vs F1ã‚¹ã‚³ã‚¢')
    plt.xlabel('Accuracy')
    plt.ylabel('F1 Score')
    plt.legend(fontsize=8)
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.suptitle('å­¦ç¿’çµæœã®åŒ…æ‹¬çš„åˆ†æ', fontsize=16, y=1.02)
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"ğŸ“Š åŒ…æ‹¬çš„åˆ†æãƒ—ãƒ­ãƒƒãƒˆã‚’ä¿å­˜: {save_path}")
    
    if show_plot:
        plt.show()
    else:
        plt.close()


def display_results_table(stats: Dict, test_results: Dict = None):
    """
    çµæœã‚’è¡¨å½¢å¼ã§è¡¨ç¤º
    
    Args:
        stats: çµ±è¨ˆé‡
        test_results: çµ±è¨ˆçš„æ¤œå®šçµæœ
    """
    print("\nğŸ“Š Cross-Validation çµæœ (å¹³å‡ Â± æ¨™æº–åå·®)")
    print("=" * 90)
    print(f"{'ãƒ¢ãƒ‡ãƒ«':<25} {'Accuracy':<12} {'Precision':<12} {'Recall':<12} {'F1-Score':<12} {'AUC':<12}")
    print("-" * 90)
    
    for model_name, model_stats in stats.items():
        row = f"{model_name:<25}"
        for metric in ['accuracy', 'precision', 'recall', 'f1', 'auc']:
            mean = model_stats[metric]['mean']
            std = model_stats[metric]['std']
            row += f"{mean:.3f}Â±{std:.3f}  "
        print(row)
    
    if test_results:
        print("\nğŸ“ˆ çµ±è¨ˆçš„æœ‰æ„æ€§æ¤œå®šçµæœ (p-value)")
        print("=" * 70)
        for comparison, metrics in test_results.items():
            model1, model2 = comparison.split('_vs_')
            print(f"\n{model1} vs {model2}:")
            for metric, result in metrics.items():
                p_value = result['p_value']
                if np.isnan(p_value):
                    print(f"  {metric}: p=N/A (ãƒ‡ãƒ¼ã‚¿ä¸è¶³)")
                else:
                    significance = "***" if p_value < 0.001 else "**" if p_value < 0.01 else "*" if p_value < 0.05 else "ns"
                    print(f"  {metric}: p={p_value:.4f} {significance}")


def save_results_to_file(results: Dict, stats: Dict, test_results: Dict,
                         output_dir: str, config: Dict):
    """
    çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    
    Args:
        results: ãƒ¢ãƒ‡ãƒ«ã”ã¨ã®è©•ä¾¡çµæœ
        stats: çµ±è¨ˆé‡
        test_results: çµ±è¨ˆçš„æ¤œå®šçµæœ
        output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        config: è¨­å®šè¾æ›¸
    """
    os.makedirs(output_dir, exist_ok=True)
    
    # çµæœã‚’JSONå½¢å¼ã§ä¿å­˜ï¼ˆnumpyå‹ã‚’å¤‰æ›ï¼‰
    results_summary = {
        'experiment_type': 'robust_cross_validation',
        'config': config,
        'raw_results': results,
        'statistics': stats,
        'statistical_tests': test_results
    }
    
    # numpyå‹ã‚’Pythonå‹ã«å¤‰æ›
    results_summary = convert_to_serializable(results_summary)
    
    output_file = os.path.join(output_dir, 'experiment_results.json')
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results_summary, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ å®Ÿé¨“çµæœã‚’ä¿å­˜: {output_file}")

